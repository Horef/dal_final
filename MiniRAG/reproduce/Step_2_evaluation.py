import os
import argparse
import pandas as pd
from datasets import Dataset

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)
from ragas.llms import llm_factory

def get_args():
    """Parses command-line arguments."""
    parser = argparse.ArgumentParser(description="Evaluate RAG results with Ragas using OpenAI")
    parser.add_argument(
        "--inputpath",
        type=str,
        required=True,
        help="Path to the CSV file generated by Step_1_QA.py",
        default="./logs/Default_output.csv"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini", # A fast and cost-effective new model
        help="OpenAI model to use for evaluation (e.g., 'gpt-4o-mini', 'gpt-4-turbo')."
    )
    parser.add_argument("--workingdir", type=str, default="./Technion")
    parser.add_argument("--datapath", type=str, default="./dataset/Technion/data/")
    return parser.parse_args()


args = get_args()


def prepare_dataset_for_ragas(df: pd.DataFrame, mode: str):
    """
    Prepares and formats the DataFrame for Ragas evaluation.
    'mode' can be 'minirag' or 'naive'.
    """
    if mode not in ['minirag', 'naive']:
        raise ValueError("Mode must be either 'minirag' or 'naive'")

    required_columns = {
        "Question": "question",
        "Gold Answer": "ground_truth",
        f"{mode}_context": "contexts",
        f"{mode}": "answer",
    }
    
    if not all(col in df.columns for col in required_columns.keys()):
        raise ValueError(f"CSV is missing one of the required columns for '{mode}' mode: {list(required_columns.keys())}")

    df_prepared = df[list(required_columns.keys())].rename(columns=required_columns)
    df_prepared['contexts'] = df_prepared['contexts'].apply(lambda x: [str(x)])
    
    df_prepared = df_prepared[df_prepared['answer'].str.strip().str.lower() != 'error']
    df_prepared = df_prepared.dropna(subset=['question', 'ground_truth', 'contexts', 'answer'])

    return Dataset.from_pandas(df_prepared)


def main():
    args = get_args()

    print(f"Loading data from: {args.inputpath}")
    try:
        results_df = pd.read_csv(args.inputpath)
    except FileNotFoundError:
        print(f"Error: The file '{args.inputpath}' was not found.")
        return

    # --- Ragas Configuration for OpenAI ---
    # IMPORTANT: You MUST set your OpenAI API key as an environment variable
    # for this script to work.
    if "OPENAI_API_KEY" not in os.environ:
        print("ðŸš¨ Error: OPENAI_API_KEY environment variable not set.")
        print("Please set your OpenAI API key to proceed with the evaluation.")
        # For example, in your terminal: export OPENAI_API_KEY='sk-...'
        return
        
    print(f"Using '{args.model}' as the evaluation model.")
    # The llm_factory will automatically use the API key from the environment
    ragas_llm = llm_factory(model=args.model)

    # Define the metrics we want to calculate.
    # Note: context_recall and context_precision can also use an LLM for more
    # semantic checks, so we pass the LLM to the evaluate function for all metrics.
    metrics = [
        faithfulness,
        answer_relevancy,
        context_recall,
        context_precision,
    ]

    # --- Evaluate 'minirag' ---
    print("\n--- Preparing 'minirag' dataset for evaluation ---")
    try:
        minirag_dataset = prepare_dataset_for_ragas(results_df, mode='minirag')
        print(f"Prepared {len(minirag_dataset)} samples for 'minirag'.")
        
        print("Running evaluation for 'minirag'...")
        # We pass the configured LLM directly to the evaluate function
        minirag_result = evaluate(minirag_dataset, metrics=metrics, llm=ragas_llm)
        minirag_result_df = minirag_result.to_pandas()
        print("\nâœ… Evaluation Complete for 'minirag'")
        print(minirag_result_df)

    except (ValueError, KeyError) as e:
        print(f"Could not evaluate 'minirag': {e}")
        minirag_result = None


    # --- Evaluate 'naive' ---
    print("\n--- Preparing 'naive' dataset for evaluation ---")
    try:
        naive_dataset = prepare_dataset_for_ragas(results_df, mode='naive')
        print(f"Prepared {len(naive_dataset)} samples for 'naive'.")

        print("Running evaluation for 'naive'...")
        naive_result = evaluate(naive_dataset, metrics=metrics, llm=ragas_llm)
        naive_result_df = naive_result.to_pandas()
        print("\nâœ… Evaluation Complete for 'naive'")
        print(naive_result_df)

    except (ValueError, KeyError) as e:
        print(f"Could not evaluate 'naive': {e}")
        naive_result = None

    # --- Final Summary ---
    print("\n\n--- ðŸ“Š Overall Score Summary ---")
    if minirag_result:
        print(f"\nMiniRAG Average Scores (Model: {args.model}):")
        print(minirag_result_df.mean())
    if naive_result:
        print(f"\nNaive RAG Average Scores (Model: {args.model}):")
        print(naive_result_df.mean())
    print("\n----------------------------------")


if __name__ == "__main__":
    main()